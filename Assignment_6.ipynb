{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bd793e-e6fc-469c-8836-bb0a9da556db",
   "metadata": {},
   "source": [
    "## Importing modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae69a06a-6f83-42dd-b0cb-f38597605f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for text clustering\n",
    "# Import stopwords to remove common words that may not be useful for clustering\n",
    "from nltk.corpus import stopwords \n",
    "# Import cosine_distance function to calculate cosine distance between vectors\n",
    "from nltk.cluster.util import cosine_distance  \n",
    "# Import numpy for numerical operations\n",
    "import numpy as np  \n",
    "# Import networkx for graph-based operations\n",
    "import networkx as nx \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f3efe-9044-4633-b49b-29f075e794bf",
   "metadata": {},
   "source": [
    "## Text 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b384de2-aba9-47cc-955d-b269a55e8371",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Japanese Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff94f8cc-5c62-42e9-8aa7-e4fe1ea1baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿受肉した御言葉の中心は、神の救い主が永遠の御父と全人類を絶え間なく愛する三重の愛の主なしるしであり象徴であると、当然かつ正当に考えられています\n",
      "それは、神が御父と聖霊と分かち合っている神の愛の象徴ですが、言葉が受肉した神だけが、弱く朽ちる体を通して現されます\n",
      "なぜなら、「神の完全性が神の内に肉体的に宿っている」からです\n",
      "さらに、それは彼の魂に注入され、キリストの人間の意志を豊かにし、至福のビジョンと直接注入されたものの両方から得られる最も完全な知識によってその行為を啓発し、統治する、その燃えるような愛の象徴です\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\pbhar\\\\Downloads\\\\Japaneese lang.txt\"\n",
    "\n",
    "# Open the file with UTF-8 encoding\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the file contents\n",
    "    filedata = file.read()\n",
    "\n",
    "# Define the regex pattern to match Japanese sentence terminators\n",
    "regex_pattern = r\"。|？|！\"\n",
    "\n",
    "# Split the text into sentences using the regex pattern\n",
    "sentences = re.split(regex_pattern, filedata)\n",
    "\n",
    "# Clean up empty sentences and remove leading/trailing spaces\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Print the sentences\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c186c7-946d-4157-9e93-8ee775334684",
   "metadata": {},
   "source": [
    "## list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c843863-6537-455d-ac28-99e1b92a562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are さらに、それは彼の魂に注入され、キリストの人間の意志を豊かにし、至福のビジョンと直接注入されたものの両方から得られる最も完全な知識によってその行為を啓発し、統治する、その燃えるような愛の象徴です\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483f215-9bd0-4fee-9f38-63c66df675d6",
   "metadata": {},
   "source": [
    "## Function to caluculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e43d1709-d84c-4771-98af-a8feb48c210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e5426-e3b1-4bd0-ac55-12f30af31e89",
   "metadata": {},
   "source": [
    "## Creating the similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c32c314-7508-4511-bb6f-6e517d04bf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.61026443 0.50311529 0.60332514]\n",
      " [0.61026443 0.         0.46556694 0.48323417]\n",
      " [0.50311529 0.46556694 0.         0.5247782 ]\n",
      " [0.60332514 0.48323417 0.5247782  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "    for idx2 in range(len(sentences)):\n",
    "        if idx1 == idx2: #ignore if both are same sentences\n",
    "            continue\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],\n",
    "sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52fd4e4-ee65-44b9-8b46-313338794a66",
   "metadata": {},
   "source": [
    "## Ranking sentences in Similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fb084ea-6114-4b88-80b4-94fa7f99cb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.2668451007030116, 1: 0.24495359805436812, 2: 0.23597296876557858, 3: 0.2522283324770418}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9919d9-0085-4418-b744-25e95c8d26f3",
   "metadata": {},
   "source": [
    "## Sort Sentences by pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43104a49-9ac9-4c4b-9059-2f9c7215f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.2668451007030116, '\\ufeff受肉した御言葉の中心は、神の救い主が永遠の御父と全人類を絶え間なく愛する三重の愛の主なしるしであり象徴であると、当然かつ正当に考えられています'), (0.2522283324770418, 'さらに、それは彼の魂に注入され、キリストの人間の意志を豊かにし、至福のビジョンと直接注入されたものの両方から得られる最も完全な知識によってその行為を啓発し、統治する、その燃えるような愛の象徴です'), (0.24495359805436812, 'それは、神が御父と聖霊と分かち合っている神の愛の象徴ですが、言葉が受肉した神だけが、弱く朽ちる体を通して現されます'), (0.23597296876557858, 'なぜなら、「神の完全性が神の内に肉体的に宿っている」からです')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in\n",
    "enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",\n",
    "ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b7982-3ae0-42f0-adbb-fe56e5681b56",
   "metadata": {},
   "source": [
    "## Pick the top 'n' sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "159777cf-dd07-4419-b23a-8dd531feb520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  1\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b80008-1588-4ec9-a051-fd039afcc044",
   "metadata": {},
   "source": [
    "## summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a76f69db-218d-417a-91a5-b36369b6d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " ﻿ 受 肉 し た 御 言 葉 の 中 心 は 、 神 の 救 い 主 が 永 遠 の 御 父 と 全 人 類 を 絶 え 間 な く 愛 す る 三 重 の 愛 の 主 な し る し で あ り 象 徴 で あ る と 、 当 然 か つ 正 当 に 考 え ら れ て い ま す\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace74cc1-b12a-4638-aad5-c00184fbc0da",
   "metadata": {},
   "source": [
    "## Text 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88aa0f-9f88-4d65-a649-9effdae2f00b",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Polish Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b047027d-8725-4517-b512-b901305cd84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Tradycyjnie firmy wykorzystywaÅ‚y swojÄ… obecnoÅ›Ä‡ w sklepach stacjonarnych, aby zrozumieÄ‡ swoich klientÃ³w â€“ jak ich przyciÄ…gnÄ…Ä‡, zaangaÅ¼owaÄ‡ i zachwyciÄ‡.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_path = r\"C:\\Users\\pbhar\\Jupiter files\\Polish.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    filedata = file.readline()  # read only the first line\n",
    "\n",
    "article = filedata.split(\". \")  # Split into sentences\n",
    "\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    cleaned_sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)  # Remove non-alphabetic characters\n",
    "    sentences.append(cleaned_sentence.split())  # Split into words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3488821-5bef-41d8-8e9f-b03523e78c7f",
   "metadata": {},
   "source": [
    "## list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b0f9f28-9471-4d3e-83e7-3d082d29d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['Tradycyjnie', 'firmy', 'wykorzystyway', 'swoj', 'obecno', 'w', 'sklepach', 'stacjonarnych', 'aby', 'zrozumie', 'swoich', 'klientw', 'jak', 'ich', 'przycign', 'zaangaowa', 'i', 'zachwyci']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b123d7-2528-433c-9696-9e339a4d87fb",
   "metadata": {},
   "source": [
    "## Function tom Caluculate the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87e7a53d-27a1-4e34-9659-aec8d364d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf57c2-b6d3-481d-b7bc-28d917d78287",
   "metadata": {},
   "source": [
    "## Creating Similarity Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0e33a3b-2086-4b93-adb1-602ce8afe227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "    for idx2 in range(len(sentences)):\n",
    "        if idx1 == idx2: #ignore if both are same sentences\n",
    "            continue\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],\n",
    "sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde70e9-7310-4b2f-bea0-aaa4db42f331",
   "metadata": {},
   "source": [
    "## Ranking Sentences in Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c8e0985d-9d3a-46be-9561-d0ffc745e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebaaf1-edb0-4ec2-ace2-1e947a89dfb8",
   "metadata": {},
   "source": [
    "## Sorting Sentences by PageRank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2704aab6-74b9-4a28-a15f-c67d357c4763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(1.0, ['Tradycyjnie', 'firmy', 'wykorzystyway', 'swoj', 'obecno', 'w', 'sklepach', 'stacjonarnych', 'aby', 'zrozumie', 'swoich', 'klientw', 'jak', 'ich', 'przycign', 'zaangaowa', 'i', 'zachwyci'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in\n",
    "enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",\n",
    "ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c335875-8c47-44d5-89de-de18ba5a4747",
   "metadata": {},
   "source": [
    "## Pick the top sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00b53872-12b4-47da-95a0-95214dd13d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  1\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edcf85e-5636-4f4e-b0cb-4aa2bfae6106",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "badd93f0-ffec-4f4a-b9e3-c922e111be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " Tradycyjnie firmy wykorzystyway swoj obecno w sklepach stacjonarnych aby zrozumie swoich klientw jak ich przycign zaangaowa i zachwyci\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bb1f2-2230-43e3-8180-2336dc697a2f",
   "metadata": {},
   "source": [
    "## Text 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e73960-f527-4c93-848d-bf70a12b46d4",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Slovak Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "15f22dd2-37dd-4fa6-bdc2-3ac3e5a4dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Firmy tradične využívajú svoju prítomnosť v kamenných predajniach, aby pochopili svojich zákazníkov – ako ich prilákať, zaujať a potešiť.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\pbhar\\Jupiter files\\Slovak.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    filedata = file.readline()  # read only the first line\n",
    "\n",
    "article = filedata.split(\". \")  # Split into sentences\n",
    "\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    cleaned_sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)  # Remove non-alphabetic characters\n",
    "    sentences.append(cleaned_sentence.split())  # Split into words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe8561-6ff8-4115-83c0-f15b939d0a31",
   "metadata": {},
   "source": [
    "## list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db4109bf-3897-4918-8a95-fa1792c79b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['Firmy', 'tradine', 'vyuvaj', 'svoju', 'prtomnos', 'v', 'kamennch', 'predajniach', 'aby', 'pochopili', 'svojich', 'zkaznkov', 'ako', 'ich', 'prilka', 'zauja', 'a', 'potei']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857d32a-3c6e-44e0-860f-958128430edd",
   "metadata": {},
   "source": [
    "## Function Calculate the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4814300f-ce9c-4274-8130-000b2a36d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45460a7a-1fa3-42d7-93e2-4cbd1ad55a28",
   "metadata": {},
   "source": [
    "## Creating the similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "06e7478e-a1b4-475e-ba3f-08c972350427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "    for idx2 in range(len(sentences)):\n",
    "        if idx1 == idx2: #ignore if both are same sentences\n",
    "            continue\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],\n",
    "sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a6187-86ee-4307-98db-4b956eba2b7c",
   "metadata": {},
   "source": [
    "## Ranking Sentences in Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c9d8fdb-438c-4540-b610-f2078bc6b7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06f183-e255-4d5a-9886-a4aa29221d0e",
   "metadata": {},
   "source": [
    "## Sorting Sentences by pageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ab85792-531b-4e0e-b760-a6c780fa9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(1.0, ['Firmy', 'tradine', 'vyuvaj', 'svoju', 'prtomnos', 'v', 'kamennch', 'predajniach', 'aby', 'pochopili', 'svojich', 'zkaznkov', 'ako', 'ich', 'prilka', 'zauja', 'a', 'potei'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in\n",
    "enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",\n",
    "ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92de0e9-77ca-4bd9-9b89-ae2e1636a5df",
   "metadata": {},
   "source": [
    "## Pick the Top Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "417065c9-f9c0-4b45-a637-f6422e411e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  1\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81acebd-60fe-4102-a84b-47467607a892",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6114422e-c0c8-407c-aac7-116d58ceec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " Firmy tradine vyuvaj svoju prtomnos v kamennch predajniach aby pochopili svojich zkaznkov ako ich prilka zauja a potei\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93cbc3-8efc-4243-981f-2e3e7adcb45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330aab65-637d-4070-bddc-b3f336e08103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
